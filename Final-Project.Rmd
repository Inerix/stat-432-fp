---
title: "How Popular is Your Music"
author: "Jonathan Lu, Carrie Wang, Zixuan Wang, Kara Wong"
date: "12/11/2019"
output:
  html_document: 
    theme: cosmo
    toc: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", autodep = TRUE, message = FALSE, warning = FALSE)
```

```{r load-packages, include = FALSE}
library("tidyverse")
library("caret")
library("knitr")
library("kableExtra")
library("randomForest")
library("gbm")
library("xgboost")
library("nnet")
library("ranger")
library("e1071")
library("glmnet")
library(doParallel)
```

***

# Abstract

> Statistical learning methods were applied to song popularity data in order to predict song popularity based on song attributes. A variety of learning techniques were explored and validated. Random forest methods show great promise in prediction ability. Due to a relatively small data size and class imbalance, a much larger dataset should be used to train models before being put into use.

***

# Introduction

In today's day and age, every one uses some music streaming platform whether it be Spotify, Apple Music, or Google Play Music. Spotify[^1] is one of the more popular service with over 217 million active monthly users worldwide[^4]. For $9.99 a year, Spotify offers users unlimited access to its 50 million + songs library. While these numbers are impressive, it's important to note that in 2018, the music industry topped \$19 billion dollars in sales. Our initial interest in doing the analysis on this data set was due to its association with music and Spotify. However, we then realized that there could definitely be some monetary implications behind this analysis due to the sheer size of the music industry.

In a dystopian future of sorts, if a model could be trained well enough, newer artists could use it as a tool to break through the extremely gate-keeping nature of the music industry as a sort of proof-of-concept in regards to their style. Since this model is being built on nothing more than the acoustic qualities of the music, it could provide an unbiased view on whether or not a song / album could be popular. If it turns out that the album is predicted to underwhelm, artists could take the hint and add in filler songs that cater to the public in an attempt to draw more attention to their profile. Adding filler songs could also boost help the artists secure funding for their next venture. Given that there are two main sides to the music industry, the artists and the labels, the music labels could also use a model like this to increase their profits by focusing their advertisements on select artists, or picking up new artists that the model deems "popular". While it seems extremely unlikely that a label would base all of their signings on a model, the model could provide them a baseline of sorts from which they can add more subjective observations.

For each song in its library, Spotify keeps track of its popularity, duration, acousticness, danceability, and many other acoustic qualities. While the algorithm by which Spotify calculates a song's popularity is hidden from the general public, it's loosely based on the amount of plays the song has recently. Spotify allows web developers and other users to query this information via their Web API[^2] which is how all the data for this dataset was accumulated.

In an attempt to construct a tool to predict song popularity based on their attributes, statistical learning techniques have been applied to song popularity data downloaded from Kaggle[^3]. The goal of this model would be to predict song popularity from song attributes for the reasons outline above. The results show even the best models built on this data failed to predict the popularity of a song. However, further improvements to these models could be explored by the addition of characteristics other than just acoustic ones.

***

# Methods

## Data

```{r read-data}
data = read.csv("data/song_data.csv")

data_cat = data %>% 
  mutate( pop_class = as.factor(case_when(
    song_popularity <= 50 ~ "F",
    song_popularity > 50 & song_popularity <= 60 ~ "E",
    song_popularity > 60 & song_popularity <= 70 ~ "D",
    song_popularity > 70 & song_popularity <= 80 ~ "C",
    song_popularity > 80 & song_popularity <= 90 ~ "B",
    song_popularity > 90 & song_popularity <= 100 ~ "A"
  )))

# Eliminate song_name from the dataset
song_data = data %>% 
  select(-song_name, -energy)

song_data_cat = data_cat %>% 
  select(-song_name, -energy)
```

```{r data-splitting}
set.seed(42)
# test-train split
idx = createDataPartition(song_data$song_popularity, p = .8, list = FALSE)
song_trn = song_data[idx, ]
song_tst = song_data[-idx, ]

song_trn_cat = song_data_cat[idx, ]
song_tst_cat = song_data_cat[-idx, ]

# create x matrix (estimation, validation, train, and test) for use with cv.glmnet()
song_trn_x = model.matrix(song_popularity~ ., data = song_trn)[, -1]
song_tst_x = model.matrix(song_popularity ~ ., data = song_tst)[, -1]

cv_trn_5 = trainControl(method = "cv", number = 5)
```

The dataset used for this analysis contains just under 19,000 songs available on Spotify and their attributes such as duration, acousticness, danceability, liveliness, and popularity. Song popularity, which is used as the response variable in this analysis, are numerical values ranging from 0 to 100, with 100 being the most popular and 0 being the least. The values are calculated by Spotify's algorithm based on the total number of plays the track has had and how recent those plays are. A full description of the each of the variables in the dataset is included in the Appendix. The data was accessed through Kaggle[^3] where it was put together by a Kaggle User who used Spotify's Web API[^2] service to obtain song attributes. Spotify has over 30 million songs in the Spotify library, so 19,000 songs contained in this dataset is just a very small subset of the data.

The data is highly imbalanced as the majority of observations have `song_popularity` value of around 60, and the values are right-skewed. 

For the purpose of this analysis, `song_name` information is eliminated because it is not a relevant feature used in the prediction. Eliminating `song_name` speeds up calculation. In preparation for model training, a training dataset is created using 80% of the provided dataï¼Œwhile the rest of the data is considered as testing data. Within the training dataset, an estimation dataset is created using 80% of the training dataset, and the remaining is constructed as validation data. 

Some exploratory data analysis can be found in the appendix.

## Modeling

In order to predict the popularity of songs, two modeling techniques were considered: linear models and tree-based models. Specifically, a simple linear model with all available features, a linear model with significant features, a random forest model, a ridge regression model, and an Xgboost model were used.

- A simple linear model using all available features fit to the training data.

```{r linear-models, echo = TRUE}
set.seed(42)
lm_1 = train(song_popularity ~ .,
             data = song_trn,
             method = "lm",
             trControl = cv_trn_5)
```

```{r lm1-rmse}
lm_1_rmse = lm_1$results[, 2]
```

- A linear regression with reduced features fit to estimation data.

```{r lm-reduced, echo = TRUE}
set.seed(42)
lm_2 = train(song_popularity ~ tempo + danceability + instrumentalness + 
               liveness + loudness + audio_valence,
             data = song_trn,
             method = "lm",
             trControl = cv_trn_5)
```

```{r lm2-rmse}
lm_2_rmse = lm_2$results[, 2]
```

- A random forest model which used 5-fold cross validation to find the optimal mtry

```{r parallel-setup}
cl = makeForkCluster(3)
registerDoParallel(cl)
```

```{r random-forest, echo = TRUE}
set.seed(42)
rf_mod = train(song_popularity ~ .,
               data = song_trn,
               trControl = cv_trn_5)
```

```{r rf-rmse}
rf_rmse = sqrt(rf_mod$mse[500])
```

- A ridge regression model which used 5-fold cross validation to find the optimal lambda.

```{r ridge-regression, echo = TRUE}
set.seed(42)
glmnet_mod = train(song_popularity ~ .,
                   data = song_trn,
                   method = "glmnet",
                   trControl = cv_trn_5,
                   tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 2, by = 0.001)))
```

```{r glmnet-rmse}
glmnet_rmse = min(glmnet_mod$results[, 3])
```

- An XGBoost model which used 5-fold cross validation to grid search for the best combination of hyper parameters.

```{r xgboost-model, echo = TRUE}
parametersGrid = expand.grid(eta = c(.1, .2, .3), 
                            colsample_bytree = c(0.5,0.7),
                            max_depth = c(3,6),
                            nrounds = 100,
                            gamma = 1,
                            min_child_weight = 2,
                            subsample = 1)

set.seed(42)
xgb_mod = train(song_popularity ~ ., data = song_trn,
                      method = "xgbTree",
                      trControl = cv_trn_5,
                      tuneGrid = parametersGrid,
                      tuneLength = 2)
```

```{r xgb_rmse}
xgb_rmse = min(xgb_mod$results[, 8])
stopImplicitCluster()
```

Models selection and evaluation is discussed in the results section.

## Evaluation

To evaluate the ability to predict song_popularity, the data was split into training and testing sets. Cross-validated training errors have been reported in this section. Our best model has been selected based on the lowest root mean-squared error; we found the best model to be our random forest model.

```{r rmse-function}
calc_rmse = function(actual, predicted) {
  sqrt(mean( (actual - predicted) ^ 2) )
}
```

```{r numeric-results}
tibble(
  "Model" = c("Simple Linear", "Reduced Linear", "Random Forest", "Ridge Regression", "XGBoost"),
  "Validation RMSE" = c(lm_1_rmse,
                        lm_2_rmse,
                        rf_rmse,
                        glmnet_rmse,
                        xgb_rmse)) %>% 
  kable(digits = 2) %>% 
  kable_styling("striped", full_width = FALSE)
```

***

# Results

The table below shows the result of song_popularity predictions on the test data using a random forest model fit to the training data.

```{r test-results}
# Predict RF on test set
pred = round(predict(rf_mod, song_tst), 0)
act = song_tst$song_popularity

# Calculate test RMSE
rf_mod_tst_rmse = calc_rmse(actual = act, predicted = pred)

# Actual vs. Predicted
gd01 = data.frame("act" = act, "pred" = pred)
gd01 %>% 
  ggplot() +
  geom_point(aes(x = act, y = pred)) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  theme(legend.position = "none") +
  ggtitle("Actual vs. Predicted (Testing Set)") + 
  xlab("Actual") +
  ylab("Predicted")

# Report test RMSE
tibble("Model" = "Random Forest Model", "Test RMSE" = rf_mod_tst_rmse) %>% 
  kable(digits = 2) %>% 
  kable_styling("striped", full_width = FALSE)
```

***

# Discussion

After evaluation, the chosen model, a random forest model, achieves a test RMSE of `r round(rf_mod_tst_rmse,2)`. This error is rather large and makes predictions relatively volatile. Additionally, due to class imbalance, a lot of the predictions are centered around 40-60 range. By examining the actual versus predicted plot of the validation data, we see that our model tends to *overpredict* popularity for songs with actual popularity less than 50, and *underpredict* popularity for songs with actual popularity greater than 50. However, we can see that as we get into the more popular songs ~ 90 - 100, we can see that our model actually predicts these values relatively well. This means that if an artist actually managed to create a chart-topping song, the model chosen through this analysis might actually be able to predict it to chop the charts. However, with how small the range of success is for this model, it's still relatively unsuccessful.

For someone using this model to determine if they should invest in an artist's song production, we see that our model would steer someone to fund a project that would actually have low success while not allocating that money into some of the songs that would actually be more popular. Given these errors, it should be noted that this model is not particularly useful in predicting song popularity. It can be improved in the following aspects. First, more data could be included to further investigation. Second, class imbalance problem should be addressed in the future. For example, a potential solution could be picking data from different popularity ranges. Let's say, 0-20, 20-40, 40-60, 60-80, 80-100, respectively. Then, subsampling an equal amount of data for each range. These ranges could also be further explored, and have one range be 0-50 to denote a classification of "unpopular songs" being less than 50. We can expect that a classification as such would have a much higher accuracy rate than our current regression.

We strongly recommend additional research to be conducted before implementing our model for its intended use.

***

# Appendix

## Data Dictionary

- `song_popularity`: A value between 0 and 100, with 100 being the most popular. Calculated by Spotify's algorithm based on total number of plays the track has had and how recent those plays are
- `song_duration`: track length in milliseconds
- `acousticness`: 0.0 to 1.0 confidence measure on how acoustic the track is, 1 representing high confidence
- `danceability`: describes how suitable a track is for dancing based on tempo, rhythm stability, beat strength, and overall regularity; 0.0 being least danceable and 1.0 most danceable
- `energy`: 0.0 to 1.0 representing perceptual measure of intensity and activity
- `instrumentalness`: 0.0 to 1.0, with 1.0 reflecting that the song contains close to no vocals
- `key`: overall key of the track; integers map to pitches using standard Pitch Class notation
- `liveness`: detects the presence of an audience in the recording
- `loudness`: overall loudness of a track in decibels
- `audio_mode`: indicates the modality (major or minor) of the track
- `speechiness`: detects presence of spoken words in the track; i.e. the more the track is like a talk show, the closer this metric is to 1.0
- `tempo`: overall estimated tempo of a track in beats per minute
- `time_signature`: an estimated number of how many beats are in each bar or measure
- `audio_valence`: describes the musical positiveness conveyed by the track 

For additional information, see documentation on Kaggle.[^3]

## EDA

```{r, Distribution-of-Song-Popularity}
p01 = song_trn %>% 
  ggplot(aes(x = song_popularity)) +
  geom_histogram(fill = "dodgerblue", bins = 30)

p02 = song_trn_cat %>% 
  ggplot(aes(x = key, col = pop_class)) +
  geom_density()

p03 = song_trn_cat %>% 
  ggplot(aes(x = tempo, col = pop_class)) +
  geom_density()

p04 = song_trn_cat %>% 
  ggplot(aes(x = song_duration_ms, col = pop_class)) +
  geom_density()

p05 = song_trn_cat %>% 
  ggplot(aes(x = liveness, col = pop_class)) +
  geom_density()

p06 = song_trn_cat %>% 
  ggplot(aes(x = time_signature, col = pop_class)) +
  geom_density()

p07 = song_trn_cat %>% 
  ggplot(aes(x = acousticness, col = pop_class)) +
  geom_density()

p08 = song_trn_cat %>% 
  ggplot(aes(x = loudness, col = pop_class)) +
  geom_density()

p09 = song_trn_cat %>% 
  ggplot(aes(x = audio_valence, col = pop_class)) +
  geom_density()

p10 = song_trn_cat %>% 
  ggplot(aes(x = danceability, col = pop_class)) +
  geom_density()

p11 = song_trn_cat %>% 
  ggplot(aes(x = audio_mode, col = pop_class)) +
  geom_density()

p12 = song_trn_cat %>% 
  ggplot(aes(x = instrumentalness, col = pop_class)) +
  geom_density()

p13 = song_trn_cat %>% 
  ggplot(aes(x = speechiness, col = pop_class)) +
  geom_density()

p01
gridExtra::grid.arrange(p02, p03, p07, p08, p09, p10, ncol = 3)
```

***

[^1]: [Wikipedia: Spotify](https://simple.wikipedia.org/wiki/Spotify)
[^2]: [Spotify: API](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/)
[^3]: [Kaggle: 19,000 Spotify Songs](https://www.kaggle.com/edalrami/19000-spotify-songs)
[^4]: [Spotify Growth numbers](https://www.theverge.com/2019/4/29/18522297/spotify-100-million-users-apple-music-podcasting-free-users-advertising-voice-speakers)
